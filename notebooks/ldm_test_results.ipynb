{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the results from the validation and testing of the latent diffusion models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import wandb\n",
    "import pickle\n",
    "import shutil\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "workdir = \"Generative Diffusion Models for 3D Geometric Objects\"\n",
    "if workdir in os.getcwd() and os.path.basename(os.getcwd()) != workdir:\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "from utils.data_processing import recalculate_holographic_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Validation of the Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a first step, the logged metrics during training and validation are downloaded from Wandb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected LDM Variants\n",
    "model_names = [\n",
    "    \"ldm_cls_8_512_e5\",\n",
    "    \"ldm_clstbl_8_512_e5\",\n",
    "    \"ldm_tbl_8_512_e5x\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subsequent code downloads the ldm metrics for the different variants and saves them as csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the wandb API\n",
    "api = wandb.Api()\n",
    "\n",
    "# Get the runs for a specific project\n",
    "entity = \"simonluder\"\n",
    "project_name = \"MSE_P8\"\n",
    "runs = api.runs(f'{entity}/{project_name}')\n",
    "run_names = [f\"{model_name}_ldm\" for model_name in model_names]\n",
    "\n",
    "# Loop through the runs and download the one with the specific name\n",
    "for run in runs:\n",
    "    \n",
    "    if run.name in run_names:\n",
    "\n",
    "        if not os.path.exists(f'holographic_pollen/{run.name.replace(\"_ldm\", \"\")}'):\n",
    "\n",
    "            print(f\"Downloading: {run.name}\")\n",
    "            run_id = run.id\n",
    "\n",
    "            # download metrics from the history artifact\n",
    "            artifact = api.artifact(name=f'{entity}/{project_name}/run-{run_id}-history:latest', type='wandb-history')\n",
    "            filename = artifact.file(\"temp/wandb/\")\n",
    "\n",
    "            \n",
    "            # create csv\n",
    "            Path(f'holographic_pollen/{run.name.replace(\"_ldm\", \"\")}').mkdir(parents=True, exist_ok=True) \n",
    "            df = pd.read_parquet(filename)\n",
    "            df.to_csv(f'holographic_pollen/{run.name.replace(\"_ldm\", \"\")}/metrics.csv', index=False)\n",
    "\n",
    "            shutil.rmtree(\"temp/wandb/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the metrics can be loaded was pandas dataframe for easy processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    df = pd.read_csv(f\"holographic_pollen/{model_name}/metrics.csv\")\n",
    "    df.loc[:, \"model\"] = model_name.replace(\"_e5x\", \"\").replace(\"_e5\", \"\")\n",
    "    df.loc[:, \"model_name\"] = model_name\n",
    "\n",
    "    metrics.append(df)\n",
    "\n",
    "metrics = pd.concat(metrics).reset_index(drop=True)\n",
    "\n",
    "metrics.head(3)\n",
    "\n",
    "# 12 epochs of trainin max\n",
    "metrics = metrics.loc[metrics[\"epoch\"] <= 12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization below shows the mean squared error between the predicted noise and the ground truth on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_metrics = metrics.loc[metrics[\"val_epoch_reconstructon_loss\"].notna()]\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.lineplot(data=validation_metrics, x=\"step\", y=\"val_epoch_reconstructon_loss\", hue=\"model\")\n",
    "plt.title(\"Mean Squared Error od the predicted noise on the validation set\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ldms were run every 10000 batches on the validation dataset. The following shows when the models performed best on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_idx = validation_metrics.groupby(\"model_name\")[\"val_epoch_reconstructon_loss\"].idxmin()\n",
    "best_val_epoch = validation_metrics.loc[min_idx]\n",
    "\n",
    "best_val_epoch = best_val_epoch[[\"model_name\", \"val_epoch_reconstructon_loss\", \"epoch\", \"_step\"]]\n",
    "best_val_epoch = best_val_epoch.rename(columns={\"model_name\":\"model\", \n",
    "                                                \"val_epoch_reconstructon_loss\":\"validation loss\",\n",
    "                                                \"_step\":\"step\"})\n",
    "\n",
    "best_val_epoch[\"step\"] = best_val_epoch[\"step\"].astype(int)\n",
    "best_val_epoch[\"epoch\"] = best_val_epoch[\"epoch\"].astype(int)\n",
    "\n",
    "best_val_epoch\n",
    "# print(best_val_epoch.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model weights of the best validation epoch were then taken for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 MSE and LPIPS Errors on the testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"ldm_cls_8_512_e5\", \n",
    "        \"ldm_clstbl_8_512_e5\", \n",
    "         \"ldm_tbl_8_512_e5x\"]\n",
    "\n",
    "df_errors = list()\n",
    "for file in files:\n",
    "    with open(f\"holographic_pollen/{file}/test/lpips_scores.pkl\", \"rb\") as f:\n",
    "        df = pd.DataFrame.from_dict(pickle.load(f))\n",
    "        df[\"model\"] = file.replace(\"_e5x\", \"\").replace(\"_e5\", \"\")\n",
    "        df_errors.append(df)\n",
    "df_errors = pd.concat(df_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "sns.boxplot(x='model', y='mse_scores', data=df_errors, flierprops={'marker': 'x', 'markersize': 4})\n",
    "plt.title('Mean squared error on generated images for the testset')\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.xlabel('model')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "print(df_errors.groupby(\"model\")[\"mse_scores\"].quantile([0.25, 0.5, 0.75]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "sns.boxplot(x='model', y='lpips_scores', data=df_errors, flierprops={'marker': 'x', 'markersize': 4})\n",
    "plt.title('LPIPS error on generated images for the testset')\n",
    "plt.ylabel('LPIPS error')\n",
    "plt.xlabel('model')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "print(df_errors.groupby(\"model\")[\"lpips_scores\"].quantile([0.25, 0.5, 0.75]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Testing the extracted visual features\n",
    "\n",
    "The visual features extracted with `skimage.measure.regionprops` are subsequently tested. For this purpose, these measured values are extracted from the test images as well as from the generated images of the LDM variants. The difference between the extracted metrics from the generated images to the test is then determined.\n",
    "\n",
    "The subsequent list shows the extracted metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement_columns = [\n",
    "    'area',\n",
    "    'bbox_area',\n",
    "    'convex_area',\n",
    "    'major_axis_length',\n",
    "    'minor_axis_length',\n",
    "    'eccentricity',\n",
    "    'solidity',\n",
    "    'perimeter',\n",
    "    'perimeter_crofton',\n",
    "    'equivalent_diameter',\n",
    "    'orientation',\n",
    "    'feret_diameter_max',\n",
    "    'max_intensity',\n",
    "    'min_intensity',\n",
    "    'mean_intensity'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to recalculate the metrics on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_gt = pd.read_csv(\"labels_test.csv\")\n",
    "df_test_gt.loc[:,\"dataset_id\"] = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_gt = recalculate_holographic_features(df_test_gt, image_path = \"data/holographic_pollen/test_images_256\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics are then recalculated from the generated images of the different ldm variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_pred_dict = dict()\n",
    "for model_name in model_names:\n",
    "    print(model_name)\n",
    "    df_test_pred = df_test_gt[[\"event_id\", \"dataset_id\", \"label\", \"rec_path\", \"filenames\", \"class_id\"]].copy(deep=True)\n",
    "    df_test_pred.loc[:,\"dataset_id\"] = \".\"\n",
    "    df_test_pred_dict[model_name] = recalculate_holographic_features(df_test_pred, image_path = f\"holographic_pollen/{model_name}/test/images/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mean Absolute Percentage Error (MAPE) and the Mean Absolute Error (MAE) between these metrics are then calculated. The MAE is only used for the evaluation orientation. As the MAPE may be outliers, images with an error in the area outside the 99.5th percentile are also removed from the evaluation (12 per model variant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_percentiles(df, groupby, cols, percentile=.99):\n",
    "    df = df.copy()\n",
    "    threshold = df.groupby(groupby)[cols].transform(lambda s: s.quantile(percentile))\n",
    "    return df[df[cols].lt(threshold).all(axis=1)], df[df[cols].ge(threshold).all(axis=1)]\n",
    "\n",
    "# def mse_error(df, columns):\n",
    "\n",
    "#     ground_truth_cols = [col_name + \"_gt\" for col_name in columns]\n",
    "#     predicted_cols = [col_name + \"_pred\" for col_name in columns]\n",
    "\n",
    "#     # Calculate squared errors for each pair of columns\n",
    "#     squared_errors = (df[ground_truth_cols].values - df[predicted_cols].values) ** 2\n",
    "\n",
    "#     df = pd.DataFrame(columns=columns, data=squared_errors)\n",
    "\n",
    "#     return df\n",
    "\n",
    "\n",
    "def mae_error(df, columns):\n",
    "\n",
    "    ground_truth_cols = [col_name + \"_gt\" for col_name in columns]\n",
    "    predicted_cols = [col_name + \"_pred\" for col_name in columns]\n",
    "\n",
    "    # Calculate squared errors for each pair of columns\n",
    "    squared_errors = (df[ground_truth_cols].values - df[predicted_cols].values)\n",
    "\n",
    "    df = pd.DataFrame(columns=columns, data=squared_errors).apply(abs)\n",
    "\n",
    "    return df\n",
    "\n",
    "def ape_error(df, columns):\n",
    "    ground_truth_cols = [col_name + \"_gt\" for col_name in columns]\n",
    "    predicted_cols = [col_name + \"_pred\" for col_name in columns]\n",
    "\n",
    "    # Calculate absolute percentage errors for each pair of columns\n",
    "    absolute_percentage_errors = np.abs((df[ground_truth_cols].values - df[predicted_cols].values) / (df[ground_truth_cols].values + 1e-9)) * 100\n",
    "\n",
    "    # Create a DataFrame to store the APE values\n",
    "    ape_df = pd.DataFrame(columns=columns, data=absolute_percentage_errors)\n",
    "\n",
    "    return ape_df\n",
    "\n",
    "\n",
    "def calculate_metrics(df_gt, df_pred, model_names, measurement_columns, metric):\n",
    "    # Initialize an empty list to store the MSE DataFrames\n",
    "    df_list = []\n",
    "\n",
    "    for model_name in model_names:\n",
    "        # Merge the ground truth and predicted DataFrames on the 'filenames' column\n",
    "        merged_df = pd.merge(df_gt, df_pred[model_name], \n",
    "                             left_on='filenames', right_on='filenames', \n",
    "                             how='outer', suffixes=[\"_gt\", \"_pred\"]).copy()\n",
    "        \n",
    "        # Calculate the MSE for the merged DataFrame\n",
    "        result = metric(merged_df, columns=measurement_columns)\n",
    "\n",
    "        result[\"event_id\"] = df_gt[\"event_id\"]\n",
    "        result[\"rec_path\"] = df_gt[\"rec_path\"]\n",
    "        result[\"dataset_id\"] = df_gt[\"dataset_id\"]\n",
    "        \n",
    "        \n",
    "        # Add the model name to the DataFrame\n",
    "        result[\"model_name\"] = model_name\n",
    "\n",
    "        # Append the DataFrame to the list\n",
    "        df_list.append(result)\n",
    "\n",
    "    # Concatenate all the MSE DataFrames\n",
    "    df = pd.concat(df_list)\n",
    "    \n",
    "    # # Clean up the model names\n",
    "    df[\"model\"] = df[\"model_name\"].apply(lambda x: x.replace(\"_e5x\", \"\").replace(\"_e5\", \"\"))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_mae = calculate_metrics(df_test_gt, df_test_pred_dict, model_names, measurement_columns, mae_error )\n",
    "# filter out rows above 99.5 percentile\n",
    "df_mae, df_mae_drop = drop_percentiles(df_mae, groupby=\"model\", cols=[\"area\"], percentile=0.995)\n",
    "\n",
    "df_mae_mean = df_mae.groupby(\"model\")[measurement_columns].mean()\n",
    "df_mae_std = df_mae.groupby(\"model\")[measurement_columns].std()\n",
    "print(len(df_mae_drop))\n",
    "\n",
    "df_ape = calculate_metrics(df_test_gt, df_test_pred_dict, model_names, measurement_columns, ape_error)\n",
    "# filter out rows above 99.5 percentile\n",
    "df_ape, df_ape_drop  = drop_percentiles(df_ape, groupby=\"model\", cols=[\"area\"], percentile=0.995)\n",
    "df_ape_mean = df_ape.groupby(\"model\")[measurement_columns].mean()\n",
    "df_ape_std = df_ape.groupby(\"model\")[measurement_columns].std()\n",
    "print(len(df_ape_drop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of transparency, these filtered out images are subsequently visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(df, num_cols=2):\n",
    "    num_rows = int(np.ceil(len(df) / num_cols))\n",
    "    fig, ax = plt.subplots(num_rows, num_cols * 2, figsize=(4 * num_cols, 3 * num_rows))\n",
    "\n",
    "    models = df[\"model\"].unique()\n",
    "    model_colors = {model: plt.cm.get_cmap(\"tab10\")(i) for i, model in enumerate(models)}\n",
    "\n",
    "    for model_idx, model in enumerate(models):\n",
    "        col_idx = (model_idx % num_cols) * 2\n",
    "        fig.text(0.5 * col_idx / num_cols + 0.5 / num_cols, 1.02, model, ha='center', fontsize=12, weight='bold', color=model_colors[model])\n",
    "\n",
    "    for i, row in df.reset_index().iterrows():\n",
    "        gt_file = os.path.join(\"data/holographic_pollen/test_images_256\", row[\"rec_path\"])\n",
    "        pred_file = os.path.join(\"holographic_pollen\", row[\"model_name\"], \"test/images\", row[\"rec_path\"])\n",
    "\n",
    "        img_gt = cv2.imread(gt_file)\n",
    "        img_pred = cv2.imread(pred_file)\n",
    "\n",
    "        col_idx = (i // num_rows) * 2\n",
    "        row_idx = i % num_rows\n",
    "\n",
    "        bg_color = model_colors[row[\"model\"]]\n",
    "\n",
    "        ax[row_idx, col_idx].imshow(img_gt)\n",
    "        ax[row_idx, col_idx].set_title(\"Ground Truth\", size=10)\n",
    "        ax[row_idx, col_idx + 1].imshow(img_pred)\n",
    "        ax[row_idx, col_idx + 1].set_title(\"Generated\", size=10)\n",
    "        \n",
    "        for axis in [ax[row_idx, col_idx], ax[row_idx, col_idx + 1]]:\n",
    "            axis.tick_params(left=False, right=False, labelleft=False, labelbottom=False, bottom=False)\n",
    "            for spine in axis.spines.values():\n",
    "                spine.set_edgecolor(bg_color)\n",
    "\n",
    "        fig.text(0.5 * col_idx / num_cols + 0.5 / num_cols, row_idx / (num_rows + 0.2) + 1 / num_rows, \n",
    "                 textwrap.fill(row[\"rec_path\"], 40, ), ha='center', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example call to the function\n",
    "plot_images(df_mae_drop, num_cols=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(df_ape_drop.groupby(\"model\").head(12), num_cols=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ape.value_counts(\"model_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mae_drop.value_counts(\"model_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table shows the MAE for the extracted metrics. The standard deviation is also shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_non_zero_decimal_pos(number):\n",
    "    # Convert the number to a string\n",
    "    num_str = str(number)\n",
    "    \n",
    "    # Find the position of the decimal point\n",
    "    decimal_position = num_str.find('.')\n",
    "    \n",
    "    # Iterate through the characters after the decimal point\n",
    "    for i, string in enumerate(num_str):\n",
    "        if string != '0' and string != '.':\n",
    "            return i - decimal_position if i > decimal_position else 0\n",
    "    \n",
    "    # If no non-zero decimal is found, return -1\n",
    "    return -1\n",
    "\n",
    "# Create an empty dataframe to store the result\n",
    "result_df = pd.DataFrame()\n",
    "# Iterate through columns\n",
    "for col in df_mae_mean.columns:\n",
    "    smallest_number = df_mae_mean[col].min()\n",
    "    dec_round = find_non_zero_decimal_pos(smallest_number)\n",
    "    # Combine elements from df1 and df2 with the desired format\n",
    "    result_df[col] = [f\"{df_mae_mean.loc[model, col].round(dec_round)}±{df_mae_std.loc[model, col].round(dec_round)}\" for model in df_mae_mean.index]\n",
    "    result_df.index = df_mae_mean.index\n",
    "    \n",
    "result_df = result_df.T\n",
    "\n",
    "result_df[\"improvement\"] = (df_mae_mean.T[\"ldm_tbl_8_512\"] / df_mae_mean.T[\"ldm_clstbl_8_512\"]).round(1)\n",
    "result_df.loc[result_df.index==\"orientation\"]\n",
    "# Display the result\n",
    "# result_df.T\n",
    "# result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And similarly with the MAPE bellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe to store the result\n",
    "result_df = pd.DataFrame()\n",
    "# Iterate through columns\n",
    "for col in df_ape_mean.columns:\n",
    "    smallest_number = df_ape_mean[col].min()\n",
    "    dec_round = find_non_zero_decimal_pos(smallest_number)\n",
    "    # Combine elements from df1 and df2 with the desired format\n",
    "    result_df[col] = [f\"{df_ape_mean.loc[model, col].round(dec_round+1)}±{df_ape_std.loc[model, col].round(dec_round+1)}\" for model in df_ape_mean.index]\n",
    "    result_df.index = df_ape_mean.index\n",
    "    \n",
    "result_df = result_df.T\n",
    "result_df\n",
    "# print(result_df.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Quantitative Evaluation\n",
    "\n",
    "The generated images are subsequently visualized in order to compare them visually with the ground truth from the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ground truth labels from the testset\n",
    "ground_truh_labels = pd.read_csv(\"labels_test.csv\")\n",
    "\n",
    "# Select samples from test set\n",
    "sample_filenames = ground_truh_labels.groupby(\"label\").head(1).sort_values(by=\"label\")[\"rec_path\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the test images\n",
    "test_results = []\n",
    "for model_name in model_names:\n",
    "    df = pd.read_csv(f\"labels_test_{model_name}.csv\")\n",
    "    df.loc[:, \"model_name\"] = model_name\n",
    "    df.loc[:, \"model\"] = model_name.replace(\"_e5x\", \"\").replace(\"_e5\", \"\")\n",
    "    df[\"dataset_id\"] = df[\"filenames\"].apply(lambda x: os.path.split(x)[0])\n",
    "    df[\"rec_path\"] = df[\"filenames\"].apply(lambda x: os.path.split(x)[1])\n",
    "    test_results.append(df)\n",
    "    \n",
    "test_results = pd.concat(test_results).reset_index(drop=True)\n",
    "\n",
    "test_results.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add ground truth labels to the dataframe\n",
    "test_results = pd.merge(test_results, ground_truh_labels[['event_id', 'rec_path', 'filenames']], on=['event_id', 'rec_path'], how='left', suffixes=(\"\", \"_gt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Number of samples to display\n",
    "num_samples = len(set(test_results[\"label\"]))\n",
    "\n",
    "# Filter the test results for the selected samples\n",
    "samples = test_results[test_results[\"rec_path\"].isin(sample_filenames)].reset_index()\n",
    "\n",
    "# Calculate the number of subplots required\n",
    "samples_per_subplot = 8  # Number of samples per subplot\n",
    "num_subplots = (num_samples + samples_per_subplot - 1) // samples_per_subplot  # Calculate the number of subplots needed\n",
    "num_cols = len(samples[samples[\"rec_path\"] == sample_filenames[0]]) + 1\n",
    "num_rows = samples_per_subplot\n",
    "\n",
    "# Iterate through each subplot\n",
    "for subplot_idx in range(num_subplots):\n",
    "    # Create a figure for each subplot\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(2 * num_cols, 2.3 * num_rows))\n",
    "    \n",
    "    # Iterate through the samples for the current subplot\n",
    "    for row_idx in range(num_rows):\n",
    "        sample_idx = subplot_idx * samples_per_subplot + row_idx\n",
    "        if sample_idx >= num_samples:\n",
    "            break\n",
    "        \n",
    "        sample_filename = sample_filenames[sample_idx]\n",
    "        # Get the subset of samples for the current filename\n",
    "        sample_subset = samples[samples[\"rec_path\"] == sample_filename].reset_index()\n",
    "        \n",
    "        # # Display the ground truth image\n",
    "        gt_path = os.path.join(\"Z:/marvel/marvel-fhnw/data/Poleno\", sample_subset.at[0, \"filenames_gt\"])\n",
    "        ground_truth_image = cv2.imread(gt_path)\n",
    "        axes[row_idx, 0].imshow(ground_truth_image)\n",
    "        axes[row_idx, 0].set_title(\"Ground Truth\", size=12)\n",
    "        axes[row_idx, 0].tick_params(left=False, right=False, labelleft=False, labelbottom=False, bottom=False)\n",
    "        \n",
    "        # Display the generated images for each sample\n",
    "        for col_idx, row in sample_subset.iterrows():\n",
    "            rel_path = f'holographic_pollen/{row[\"model_name\"]}/test/images/{row[\"rec_path\"]}'\n",
    "            generated_image = cv2.imread(rel_path)\n",
    "            axes[row_idx, col_idx + 1].imshow(generated_image)\n",
    "            axes[row_idx, col_idx + 1].set_title(row[\"model\"], size=12)\n",
    "            axes[row_idx, col_idx + 1].tick_params(left=False, right=False, labelleft=False, labelbottom=False, bottom=False)\n",
    "        \n",
    "        # Set the label as subtitle for each row\n",
    "        fig.text(-0.1, (num_rows - row_idx - 0.5) / num_rows, sample_subset.at[0, \"label\"], ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Adjust layout and show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Siamese Classifier Results\n",
    "\n",
    "#### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first the validation accuracy of the siamese classifier during training is visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the wandb API\n",
    "api = wandb.Api()\n",
    "\n",
    "# Get the runs for a specific project\n",
    "entity = \"simonluder\"\n",
    "project_name = \"MSE_P8\"\n",
    "runs = api.runs(f'{entity}/{project_name}')\n",
    "run_names = [f\"siamese_classifier_genus_l\" ]\n",
    "\n",
    "# Loop through the runs and download the one with the specific name\n",
    "for run in runs:\n",
    "    \n",
    "    if run.name in run_names:\n",
    "\n",
    "        print(f\"Downloading: {run.name}\")\n",
    "        run_id = run.id\n",
    "\n",
    "        # download metrics from the history artifact\n",
    "        artifact = api.artifact(name=f'{entity}/{project_name}/run-{run_id}-history:latest', type='wandb-history')\n",
    "        filename = artifact.file(\"temp/wandb/\")\n",
    "\n",
    "        # create csv\n",
    "        Path(f'holographic_pollen/{run.name}').mkdir(parents=True, exist_ok=True) \n",
    "        df = pd.read_parquet(filename)\n",
    "        df.to_csv(f'holographic_pollen/{run.name}/metrics.csv', index=False)\n",
    "\n",
    "        shutil.rmtree(\"temp/wandb/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "\n",
    "for model_name in run_names:\n",
    "    df = pd.read_csv(f\"holographic_pollen/{model_name}/metrics.csv\")\n",
    "    df.loc[:, \"model\"] = model_name.replace(\"_l\", \"\")\n",
    "    df.loc[:, \"model_name\"] = model_name\n",
    "\n",
    "    metrics.append(df)\n",
    "\n",
    "metrics = pd.concat(metrics).reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.lineplot(data=metrics, x=\"step\", y=\"val_accuracy\", hue=\"model\")\n",
    "plt.title(\"Classification Accuracy on the validation set\")\n",
    "plt.ylabel(\"Mean Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ground_truth = pd.read_csv(\"labels_test.csv\")\n",
    "df_ground_truth[\"genus\"] = df_ground_truth[\"label\"].apply(lambda x: x.split()[0])\n",
    "id_to_genus = df_ground_truth.drop_duplicates().set_index('genus_id')['genus'].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variants = [\"test\", \"ldm_cls_8_512_e5\", \"ldm_clstbl_8_512_e5\"]\n",
    "# variants = [\"test\", \"test_256\", \"test_256_16\", \"validation\", \"validation_256_16\", \"ldm_cls_8_512_e5\", \"ldm_clstbl_8_512_e5\", \"ldm_tbl_8_512_e5x\"]\n",
    "variants = [ \"test_256\", \"ldm_cls_8_512_e5\", \"ldm_clstbl_8_512_e5\", \"ldm_tbl_8_512_e5x\"]\n",
    "# classifier = \"resnet_classifier_005\"\n",
    "classifier = \"siamese_classifier_genus_l\"\n",
    "# classifier = \"siamese_classifier_mini\"\n",
    "# classifier = \"siamese_classifier_genus_complete\"\n",
    "ground_truth = \"test_256\"\n",
    "\n",
    "labels = dict()\n",
    "predictions = dict()\n",
    "\n",
    "df_predictions = []\n",
    "\n",
    "for variant in variants:\n",
    "    # labels_dir = f\"holographic_pollen/{classifier}/validation/{9999}/labels.npy\"\n",
    "    # predictions_dir = f\"holographic_pollen/{classifier}/validation/{9999}/predictions.npy\"\n",
    "    labels_dir = f\"holographic_pollen/{classifier}/test/{variant}/labels.npy\"\n",
    "    predictions_dir = f\"holographic_pollen/{classifier}/test/{variant}/predictions.npy\"\n",
    "    gt_dir = f\"holographic_pollen/{classifier}/test/{ground_truth}/predictions.npy\"\n",
    "    # labels_dir = f\"C:/Users/simon/Downloads/{classifier}/test/{variant}/labels.npy\"\n",
    "    # predictions_dir = f\"C:/Users/simon/Downloads/{classifier}/test/{variant}/predictions.npy\"\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame({\"true_id\":np.load(labels_dir), \"pred_id\":np.load(predictions_dir), \"equal_pred\":np.load(predictions_dir)==np.load(gt_dir)})\n",
    "    \n",
    "    df[\"variant\"] = variant.replace(\"_256\", \"\").replace(\"_16\", \"\").replace(\"_e5x\", \"\").replace(\"_e5\", \"\")\n",
    "    df[\"true_genus\"] = df[\"true_id\"].apply(lambda x: id_to_genus[x])\n",
    "    df[\"pred_genus\"] = df[\"pred_id\"].apply(lambda x: id_to_genus[x])\n",
    "\n",
    "    ground_truth\n",
    "    \n",
    "\n",
    "    df_predictions.append(df)\n",
    "\n",
    "df_predictions = pd.concat(df_predictions)\n",
    "df_predictions[\"correct\"] = df_predictions[\"pred_id\"] == df_predictions[\"true_id\"]\n",
    "df_predictions.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"C:/Users/simon/Downloads/\"\n",
    "# classifier  = \"siamese_classifier_genus_complete\"\n",
    "\n",
    "# variants = os.listdir(f\"{path}/{classifier}/validation\")\n",
    "# for ckpt in os.listdir(f\"{path}/{classifier}/validation\"):\n",
    "\n",
    "#     labels_dir = f\"{path}/{classifier}/validation/{ckpt}/labels.npy\"\n",
    "#     predictions_dir = f\"{path}/{classifier}/validation/{ckpt}/predictions.npy\"\n",
    "#     labels[ckpt] = np.load(labels_dir)\n",
    "#     predictions[ckpt] = np.load(predictions_dir)\n",
    "\n",
    "# ckpt = \"7800\"\n",
    "# (labels[ckpt] == predictions[ckpt]).sum() / len(labels[ckpt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_predictions.groupby(\"variant\")[\"correct\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_predictions\n",
    "\n",
    "# Number of variants\n",
    "variants = list(df[\"variant\"].drop_duplicates())\n",
    "print(variants)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, len(variants), figsize=(8 * len(variants), 6))\n",
    "\n",
    "for idx, variant in enumerate(variants):\n",
    "    sub_df = df.loc[df[\"variant\"]==variant]\n",
    "    conf_matrix = confusion_matrix(sub_df[\"true_id\"], sub_df[\"pred_id\"], normalize=\"true\")\n",
    "\n",
    "    sub_df\n",
    "    labels = list(sub_df.sort_values(\"true_id\")[\"true_genus\"].drop_duplicates())\n",
    "\n",
    "    df_cm = pd.DataFrame(data=conf_matrix, index=labels, columns=labels)\n",
    "\n",
    "    ax = axes[idx]\n",
    "    sns.heatmap(df_cm, annot=False, fmt='g', cmap='Blues', cbar=False, ax=ax, vmin=0, vmax=1)\n",
    "    ax.set_title(variant)\n",
    "    ax.set_xlabel('Predicted')\n",
    "\n",
    "    if idx == 0:\n",
    "        ax.set_ylabel('True')\n",
    "\n",
    "    if idx == len(variants)-1:\n",
    "        cbar = fig.colorbar(axes[0].collections[0], ax=axes, location='right', pad=0.01)\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, variant in enumerate(reversed(variants)):\n",
    "    print(variant)\n",
    "    sub_df = df.loc[df[\"variant\"]==variant]\n",
    "    print(classification_report(sub_df[\"true_id\"], sub_df[\"pred_id\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "three_D",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
